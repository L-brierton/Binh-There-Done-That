{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Assignment 2\n",
    "COMPETITION TASK: \n",
    "\n",
    "+ Learn the classification model for training set with 5 categorical data from ['business', 'entertainment', 'politics', 'sport', 'tech'].\n",
    "\n",
    "+ Apply learned model to get the labels for \"testdata.csv\"\n",
    "\n",
    "## Team Members: \n",
    "Laura Brierton - 15317451, Clodagh Lalor - 13354426, Jeremy Schiff - student#, Peter Concannon - student#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category\n",
       "0  French boss to leave EADS The French co-head o...       business\n",
       "1  Gamers could drive high-definition TV, films, ...           tech\n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics\n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment\n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_trainset = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "raw_testdata = pd.read_csv('testdata.csv',sep='^',header=0)\n",
    "raw_trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Function to convert raw text to tokens\n",
    "def convert_tokens(rawtext, verbose):\n",
    "    # First: Tokenization\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_words = tokenizer.tokenize(rawtext)\n",
    "    if (verbose):\n",
    "        print('Tokens:' + str(token_words[0:10]))\n",
    "    \n",
    "    # # Second: Decapitalization (if needed)\n",
    "    # decap_token_words = [word.lower() for word in token_words]\n",
    "    # print('Decapitalized Tokens:' + str(decap_token_words[0:10]))\n",
    "    \n",
    "    # Third: Remove stop words\n",
    "    json_data=open('stopwords.json', encoding=\"utf8\").read()\n",
    "    stopwords_json = json.loads(json_data)\n",
    "    stopwords_json_en = set(stopwords_json['en'])\n",
    "    stopwords_nltk_en = set(stopwords.words('english'))\n",
    "    # Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "    stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en)\n",
    "\n",
    "    rmsw_token_words = ([word for word in token_words if word.lower() not in stoplist_combined])\n",
    "    if (verbose):\n",
    "        print('Stopwords removed:' + str(rmsw_token_words[0:20]))\n",
    "    \n",
    "    ## Fouth: remove CAP words\n",
    "    rmcap_token_words =[]\n",
    "    for word in rmsw_token_words:\n",
    "        if word.isupper():\n",
    "            rmcap_token_words.append(word.title())\n",
    "        else:\n",
    "            rmcap_token_words.append(word)\n",
    "    if (verbose):\n",
    "        print('CAPITALIZED removed:' + str(rmcap_token_words[0:20]))\n",
    "        \n",
    "     ## Fifth : Remove salutation\n",
    "    salutation = ['mr','mrs','mss','dr','phd','prof','rev', 'professor']\n",
    "    rmsalu_token_words = ([word for word in rmcap_token_words if word.lower() not in salutation])\n",
    "    if (verbose):\n",
    "        print('Salutation removed:' + str(rmsalu_token_words[0:20]))\n",
    "        \n",
    "     ## Sixth: Remove Numbers\n",
    "    rmnb_token_words = ([word for word in rmsalu_token_words if not word.isdigit()])\n",
    "    if (verbose):\n",
    "        print('Number removed: ' + str(rmnb_token_words[0:20]))\n",
    "        \n",
    "    ## define transfer tag function:\n",
    "    def transfer_tag(treebank_tag):\n",
    "        if treebank_tag.startswith('j' or 'J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('v' or 'V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('n' or 'N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('r' or 'R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return 'n'\n",
    "    \n",
    "    ## Seventh: Lemmatization\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(rmnb_token_words):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        if not wtag:\n",
    "            lemma_words.extend([word])\n",
    "        else:\n",
    "            lemma_words.extend([wnl.lemmatize(word, wtag)]) # -> get lemma for word with tag\n",
    "    if (verbose):\n",
    "        print('Lemmas : ' + str(lemma_words[0:10]))\n",
    "        \n",
    "    \n",
    "    ## RETURN\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle = raw_trainset.copy()\n",
    "[n,d] = df_handle.shape\n",
    "df_handle['Tokens'] = ['']*n\n",
    "\n",
    "for index, row in df_handle.iterrows():\n",
    "    df_handle['Tokens'].iloc[index] = convert_tokens(row['content'],0)\n",
    "    \n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
