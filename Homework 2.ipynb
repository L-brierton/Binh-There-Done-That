{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Assignment 2\n",
    "COMPETITION TASK: \n",
    "\n",
    "+ Learn the classification model for training set with 5 categorical data from ['business', 'entertainment', 'politics', 'sport', 'tech'].\n",
    "\n",
    "+ Apply learned model to get the labels for \"testdata.csv\"\n",
    "\n",
    "## Team Members: \n",
    "Laura Brierton - 15317451, Clodagh Lalor - 13354426, Jeremy Schiff - student#, Peter Concannon - student#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a jupyter notebook extension to create a table of contents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk, json\n",
    "from wordcloud import WordCloud\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by importing our training and test datasets and placing them into dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category\n",
       "0  French boss to leave EADS The French co-head o...       business\n",
       "1  Gamers could drive high-definition TV, films, ...           tech\n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics\n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment\n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_trainset = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "raw_testdata = pd.read_csv('testdata.csv',sep='^',header=0)\n",
    "raw_trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the below function to tokenise our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Function to convert raw text to tokens\n",
    "def convert_tokens(rawtext, verbose=False):\n",
    "    # First: Tokenization\n",
    "    # start by removing hyphens to allow for better tokenization\n",
    "    rawtext = rawtext.replace('-', ' ')\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_words = tokenizer.tokenize(rawtext)\n",
    "    if (verbose):\n",
    "        print('Tokens:' + str(token_words[0:10]))\n",
    "    \n",
    "    # Second: Decapitalization \n",
    "    decap_token_words = [word.lower() for word in token_words]\n",
    "    if (verbose):\n",
    "        print('Decapitalized Tokens:' + str(decap_token_words[0:10]))\n",
    "    \n",
    "    # Third: Remove stop words\n",
    "    json_data=open('stopwords.json', encoding=\"utf8\").read()\n",
    "    stopwords_json = json.loads(json_data)\n",
    "    stopwords_json_en = set(stopwords_json['en'])\n",
    "    stopwords_nltk_en = set(stopwords.words('english'))\n",
    "    # Combine the stopwords\n",
    "    stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en)\n",
    "\n",
    "    \n",
    "    rmsw_token_words = ([word for word in decap_token_words if word.lower() not in stoplist_combined])\n",
    "    if (verbose):\n",
    "        print('Stopwords removed:' + str(rmsw_token_words[0:20]))\n",
    "    \n",
    "    ## Fouth: remove CAP words\n",
    "    rmcap_token_words =[]\n",
    "    for word in rmsw_token_words:\n",
    "        if word.isupper():\n",
    "            rmcap_token_words.append(word.title())\n",
    "        else:\n",
    "            rmcap_token_words.append(word)\n",
    "    if (verbose):\n",
    "        print('CAPITALIZED removed:' + str(rmcap_token_words[0:20]))\n",
    "        \n",
    "     ## Fifth : Remove salutation\n",
    "    salutation = ['mr','mrs','mss','dr','phd','prof','rev', 'professor']\n",
    "    rmsalu_token_words = ([word for word in rmcap_token_words if word.lower() not in salutation])\n",
    "    if (verbose):\n",
    "        print('Salutation removed:' + str(rmsalu_token_words[0:20]))\n",
    "        \n",
    "     ## Sixth: Remove words containing numbers\n",
    "    rmnb_token_words = ([word for word in rmsalu_token_words if not re.search(r\"\\d+\", word)])\n",
    "    if (verbose):\n",
    "        print('Number removed: ' + str(rmnb_token_words[0:20]))\n",
    "        \n",
    "    ## define transfer tag function:\n",
    "    def transfer_tag(treebank_tag):\n",
    "        if treebank_tag.startswith('j' or 'J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('v' or 'V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('n' or 'N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('r' or 'R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return 'n'\n",
    "    \n",
    "    ## Seventh: Lemmatization\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(rmnb_token_words):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        if not wtag:\n",
    "            lemma_words.extend([word])\n",
    "        ##please note we had to hardcode the following words in due to an error with word net\n",
    "        elif word == \"boss\":\n",
    "            lemma_words.extend([(word)])\n",
    "        elif word == \"gamers\":\n",
    "            lemma_words.extend([(\"gamer\")])\n",
    "        else:\n",
    "            lemma_words.extend([wnl.lemmatize(word, wtag)]) # -> get lemma for word with tag\n",
    "    if (verbose):\n",
    "        print('Lemmas : ' + str(lemma_words[0:10]))\n",
    "        \n",
    "    \n",
    "    ## RETURN\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenized our training and test set separately\n",
    "<br>\n",
    "<br>\n",
    "Extract tokens for training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we next create a dataframe that contained the content category and bag of words for each document\n",
    "df_handle = raw_trainset.copy()\n",
    "df_handle[\"Tokens\"] = df_handle.apply(lambda row: convert_tokens(row[\"content\"]), axis=1)\n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tokens for test set:\n",
    "<br>\n",
    "* Note how we dont have the category column in this dataset, as this is what we want our model to ultimately predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle_test = raw_testdata.copy()\n",
    "df_handle_test_onhold = raw_testdata.copy() #on hold until the end of the document\n",
    "df_handle_test[\"Tokens\"] = df_handle_test.apply(lambda row: convert_tokens(row[\"content\"]), axis=1)\n",
    "df_handle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continued the tokenizing process and created entries that contain only the noun or only the adjective tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generalization of the extraction\n",
    "def extract_pos_tokens(tokens, pos):\n",
    "    # helper for list comprehension\n",
    "    def is_pos(treebank_tag):\n",
    "        if treebank_tag.startswith(pos):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return [word for (word, tag) in nltk.pos_tag(tokens) if is_pos(tag)]\n",
    "\n",
    "#Specific noun instance\n",
    "def extract_noun_tokens(tokens):\n",
    "    # note that this does not include the \"or 'n'\" component which was both unnecessary and didnt work on my machine\n",
    "    # furthermore, it does not take noun to be the default\n",
    "    return extract_pos_tokens(tokens, 'N')\n",
    "    \n",
    "#Specific adjective instance\n",
    "def extract_adj_tokens(tokens):\n",
    "    # same idea as with nounds - the or 'j' is unneeded\n",
    "    return extract_pos_tokens(tokens, 'J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step adds our noun_token and adjective_token columns to our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle[\"noun_tokens\"] = df_handle.apply(lambda row: extract_noun_tokens(row[\"Tokens\"]), axis=1)\n",
    "df_handle[\"adjective_tokens\"] = df_handle.apply(lambda row: extract_adj_tokens(row[\"Tokens\"]), axis=1)\n",
    "\n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we do the same for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle_test[\"noun_tokens\"] = df_handle_test.apply(lambda row: extract_noun_tokens(row[\"Tokens\"]), axis=1)\n",
    "df_handle_test[\"adjective_tokens\"] = df_handle_test.apply(lambda row: extract_adj_tokens(row[\"Tokens\"]), axis=1)\n",
    "\n",
    "df_handle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deconstruction - Wordclouds and Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decided to create a wordcloud for the entire corpus, to get an idea of the most common words and if there was any common pattern. We thought that it might help us to decide if there were any more pre-processing steps we needed to take before moving onto our Analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud function\n",
    "def wordcloudplot(tokens, name):\n",
    "    \n",
    "    text2 = ' '.join(tokens)\n",
    "\n",
    "    wordcloud = WordCloud(width=1600, height=800).generate(text2)\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    # save to file if filename given\n",
    "    if name:\n",
    "        wordcloud.to_file(name)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined tokens refers to all the tokens from all the documents in the training data dataframe\n",
    "joined_tokens = [token for document in df_handle[\"Tokens\"] for token in document]\n",
    "\n",
    "#saves wordcloud of all tokens as file. Please note, this word cloud is for all tokens not just noun tokens\n",
    "wordcloudplot(joined_tokens, 'img_wordcloud1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our word cloud, we can see immediately that a lot of verbs are present, this will not necessarily help us with our classification step and so this influenced us to look at noun and adjective tokens instead, going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_tokens = [token for document in df_handle[\"noun_tokens\"] for token in document]\n",
    "adjective_tokens = [token for document in df_handle[\"adjective_tokens\"] for token in document]\n",
    "wordcloudplot(noun_tokens, 'img_wordcloud2.png')\n",
    "wordcloudplot(adjective_tokens, 'img_wordcloud3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these separate wordclouds separately gives us a nice overview of our trainin dataset. We can see a good reflection of the known classes used. It indicates to us a pretty balanced dataset as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on this, we wanted to look at the frequency of certain nouns and adjectives overall in the data with hopes that we could glean some information to aid classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_noun_tokens = [token for document in df_handle[\"noun_tokens\"] for token in document]\n",
    "word_frequency = nltk.FreqDist(joined_noun_tokens)\n",
    "word_frequency.plot(20, title='Twenty Most Common Nouns')\n",
    "\n",
    "joined_adjective_tokens = [token for document in df_handle[\"adjective_tokens\"] for token in document]\n",
    "word_frequency = nltk.FreqDist(joined_adjective_tokens)\n",
    "word_frequency.plot(20, title='Twenty Most Common Adjectives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nouns looks pretty useful with words like government, film, and game all likely being high indicators as to the category of the data. On the other hand, the adjectives seem much less useful with words like high, big, and good being so prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Most Common Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of our analysis involved looking at bigrams and trigrams, which we thought might aid us in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tokens = [token for document in df_handle[\"Tokens\"] for token in document]\n",
    "\n",
    "bigram = ngrams(joined_tokens, 2)\n",
    "bi_frequencies = nltk.FreqDist(bigram)\n",
    "dict_items =list(dict(bi_frequencies).items())\n",
    "#make a dataframe of the bigrams and their frequencies\n",
    "df_bigramFreq = pd.DataFrame(dict_items, columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "df_bigramFreq = df_bigramFreq.reset_index(drop=True)\n",
    "#show only top five\n",
    "df_bigramFreq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the gram is noun gram or not\n",
    "def IsNounGram(ngram):\n",
    "    if ('-pron-' in ngram) or ('t' in ngram):\n",
    "        return False\n",
    "    \n",
    "    first_type = ('JJ','JJR','JJS','NN','NNS','NNP','NNPS')\n",
    "    second_type = ('NN','NNS','NNP','NNPS')\n",
    "    tags = nltk.pos_tag(ngram,lang='eng')\n",
    "    if (tags[0][1] in first_type) and (tags[1][1] in second_type):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we decided to look only at top 50 for efficiency sake\n",
    "df_bigramFreq_filter = df_bigramFreq.copy().head(50)\n",
    "#create a new column which check for each bigram, which are noun grams , returns true or false\n",
    "df_bigramFreq_filter['noun_gram'] = df_bigramFreq_filter[\"bigram\"].map(lambda x : IsNounGram(x))\n",
    "#filter out those that are false\n",
    "df_bigramFreq_filter = df_bigramFreq_filter[df_bigramFreq_filter.noun_gram != False]\n",
    "df_bigramFreq_filter.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that our top five have changed a little (as has the entire dataframe). We were concerned with the (tell, BBC) bigram, and so decided that we needed to add an additional step of checking that this appeared in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check that these ngrams actually appear together in the text!\n",
    "def CheckWordInText(word, Text):\n",
    "    if word in Text.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all texts\n",
    "full_corpus =' '.join([document for document in df_handle[\"content\"]])\n",
    "# combine all the tokens\n",
    "for index, row in df_handle.iterrows():\n",
    "    full_corpus = full_corpus + df_handle['content'].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this step adds a column stating that the n-gram appears as is, in the text\n",
    "df_real_bigram = df_bigramFreq_filter.copy()\n",
    "\n",
    "#true if exists\n",
    "exists_list = []\n",
    "for index, row in df_bigramFreq_filter.iterrows():\n",
    "    gram = row['bigram']\n",
    "    word = (' '.join(gram))\n",
    "    exists_list.append(CheckWordInText(word, full_corpus))\n",
    "\n",
    "#create new column for these    \n",
    "df_bigramFreq_filter['exist'] = exists_list    \n",
    "\n",
    "#delete those that are false\n",
    "df_real_bigram = df_bigramFreq_filter[df_bigramFreq_filter.exist != False]\n",
    "\n",
    "#show top 5\n",
    "df_real_bigram.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now do the same for trigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tokens = [token for document in df_handle[\"Tokens\"] for token in document]\n",
    "trigram = ngrams(joined_tokens, 3)\n",
    "tri_frequencies = nltk.FreqDist(trigram)\n",
    "dict_items = list(dict(tri_frequencies).items())\n",
    "#make a dataframe of the trigrams and their frequencies\n",
    "df_trigramFreq = pd.DataFrame(dict_items, columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
    "df_trigramFreq = df_trigramFreq.reset_index(drop=True)\n",
    "\n",
    "#check for noun grams\n",
    "##look only at top 50 for efficiency sake\n",
    "df_trigramFreq_filter = df_trigramFreq.copy().head(50)\n",
    "#creaete a new column which check for each bigram, which are noun grams , returns true or false\n",
    "df_trigramFreq_filter['noun_gram'] = df_trigramFreq_filter[\"trigram\"].map(lambda x : IsNounGram(x))\n",
    "#filter out those that are false\n",
    "df_trigramFreq_filter = df_trigramFreq_filter[df_trigramFreq_filter.noun_gram != False]\n",
    "#printf(df_bigramFreq_filter.head(5))\n",
    "\n",
    "#check in text\n",
    "df_real_trigram = df_trigramFreq_filter.copy()\n",
    "\n",
    "exits_list = []\n",
    "for index, row in df_trigramFreq_filter.iterrows():\n",
    "    gram = row['trigram']\n",
    "    word = (' '.join(gram))\n",
    "    exits_list.append(CheckWordInText(word, full_corpus))\n",
    "    \n",
    "df_real_trigram['exist'] = exits_list\n",
    "df_real_trigram = df_real_trigram.loc[df_real_trigram.exist==True]\n",
    "\n",
    "\n",
    "#df_trigramFreq_filter.head(5)\n",
    "df_real_trigram.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the results from our bigrams and trigrams, we decided **against** using them for classification purposes. Looking at the top 5 of both, we thought that they were mostly only relevant for the politics class and therefore by using bigrams and trigrams as part of our model we may bias, what looks to be, a fairly balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF vectorisation is used for identifying significant words within the dataset, giving greater wieght to words that are uncommon across the full corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Vectorising the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF vectorisation is used for identifying significant words within documents, giving greater weight to words that are especially common in an individual document while being uncommon across the full corpus. By doing so, we can get an idea of what words are especially important within individual documents, and use this information to better classify documents in the corpus.\n",
    "\n",
    "The structure of the TF-IDF vectors must constant in order to use them for most models. Because of this, words will be present in the TF-IDF vectors if and only if they appear in the training data (after normalisation). Therefore, certain words which would have had greater weights in documents in the test dataset will be discarded. This should have minimal impact on our model's ability to classify with TF-IDF, as the absense of those words from the training set means that the models will have no known values for the new words, and will therefore be unable to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the sample output given shows only zeros but it is working\n",
    "merged_tokens = [\" \".join(x) for x in df_handle[\"noun_tokens\"]]\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_train = pd.DataFrame(tfidf_vectorizer.fit_transform(merged_tokens).todense(), columns = tfidf_vectorizer.get_feature_names())\n",
    "tfidf_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Vectorising the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_data = [\" \".join(x) for x in df_handle_test.Tokens]\n",
    "\n",
    "# Create a tfidf for the test data with the same dimensions (i.e. headers)\n",
    "# as the set above. necessary for comparing models\n",
    "tfidf_clean = tfidf_vectorizer.transform(test_clean_data)\n",
    "## For printing that tf-idf matrix, we convert it into dataframe\n",
    "tfidf_test = pd.DataFrame(tfidf_clean.toarray(),columns=[tfidf_vectorizer.get_feature_names()])\n",
    "tfidf_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Class Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes our known labels from the dataframe to use for training our model\n",
    "labels_train = df_handle['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to plot the frequency of the labels to ensure that we had a fairly balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique, counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(unique,counts)\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see this is a balanced dataset, and this reinforced our decision not to use bigrams and trigrams, especially as politics is not the most frequent class and as such we could bias our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Select Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Cross Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.1\n",
    "num_folds = 10\n",
    "model_tfidf, validation_tfidf, model_labels, validation_labels = train_test_split(tfidf_train, labels_train, test_size=validation_size, shuffle=True, stratify=labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAYES classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbayes = MultinomialNB()\n",
    "modelbayes.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier()\n",
    "randomforest.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelkmeans = KMeans(n_clusters=5, init='k-means++', max_iter=200, n_init=100)\n",
    "modelkmeans.fit(model_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we attempt to use GridSearchCV to identify the ideal value for k in the kNN classifier. Calculating this hyperparameter is costly, as it requires the creation of a kNN model for each possible value in the given set. Although the set of possible values here is kept small in our submission notebook for the sake of running faster, we did run tests on the range \\[1, 2, 3, 4, 5\\], and still found that 1 was the optimal value for accuracy. \n",
    "\n",
    "The difference between documents of differing classification is small enough that comparing to more than the singular nearest neighbour results in more incorrect classifications. Using k=1 may be a case of overfitting the data: a lower value for k gives a more complex model with lower bias and higher variance. However, this is not likely to be an issue, as our use of ensemble classification will help to greatly reduce the variance of the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#takes about 5-6 mins to complete\n",
    "# we only used 1 to 2 here as this step takes so long. Please note that we had tried for [1,..,5] but after it ran for about 30 mins, determined that the optimum is actually within the range [1,2] so dicided to fo with this for efficiencies sake\n",
    "k_values = [1,2]\n",
    "print(\"the range of k is \" + str(k_values))\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), {'n_neighbors':k_values}, scoring='accuracy', cv = num_folds)\n",
    "grid_search.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step will take the optimum k value determined in the previous step and use that for our k neighbours classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelknn = KNeighborsClassifier(**grid_search.best_params_)\n",
    "modelknn.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this subset, the best value is k=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1, 1e2]\n",
    "grid_search = GridSearchCV(LogisticRegression(), {'C':c_values}, scoring='accuracy', cv = num_folds)\n",
    "grid_search.fit(model_tfidf, model_labels)\n",
    "\n",
    "modellr = LogisticRegression(**grid_search.best_params_)\n",
    "modellr.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Applying models to test set to estimate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decided to apply our models to our partitioned training set to see how they performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_bayes = modelbayes.predict(validation_tfidf)\n",
    "predicted_probas_bayes = modelbayes.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_randomforest = randomforest.predict(validation_tfidf)\n",
    "predicted_probas_randomforest = randomforest.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_knn = modelknn.predict(validation_tfidf)\n",
    "predicted_probas_knn = modelknn.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_lr = modellr.predict(validation_tfidf)\n",
    "predicted_probas_lr = modellr.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans requires a little more work here as it is an unsupervised algorithm. The following predicts the clusters, but currently they are labelled 0-4 and as such we need to set these clusters as actually category labels so that we can discover how well this method has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_kmeans = modelkmeans.predict(validation_tfidf)\n",
    "predicted_labels_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the corresponding names for the clusters I applied the kmeans to the whole set and looked at the most common category for each. I quickly learned that the number of iterations i would need to make on my model to make it provide even sized clusters that I could confidently label as the five categories, simply took far too much time to make this an optimal model to use in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelkmeans = KMeans(n_clusters=5, init='k-means++')\n",
    "modelkmeans.fit(tfidf_train)\n",
    "all_predicted_label = modelkmeans.predict(tfidf_train)\n",
    "newdf = df_handle.copy()\n",
    "newdf[\"Cluster\"] = all_predicted_label\n",
    "newdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0df = newdf.loc[newdf['Cluster'].isin(['0'])]\n",
    "cluster1df = newdf.loc[newdf['Cluster'].isin(['1'])]\n",
    "cluster2df = newdf.loc[newdf['Cluster'].isin(['2'])]\n",
    "cluster3df = newdf.loc[newdf['Cluster'].isin(['3'])]\n",
    "cluster4df = newdf.loc[newdf['Cluster'].isin(['4'])]\n",
    "print(\"Cluster sizes: \")\n",
    "print(\"Cluster 0: \" + str(cluster0df.size))\n",
    "print(\"Cluster 1: \" + str(cluster1df.size))\n",
    "print(\"Cluster 2: \" + str(cluster2df.size))\n",
    "print(\"Cluster 3: \" + str(cluster3df.size))\n",
    "print(\"Cluster 4: \" + str(cluster4df.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we can see percentages for the accuracy rates for our model for each of the classifer methods used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_bayes = accuracy_score(validation_labels, predicted_labels_bayes)\n",
    "Acc_ranfor = accuracy_score(validation_labels, predicted_labels_randomforest)\n",
    "Acc_knn = accuracy_score(validation_labels, predicted_labels_knn)\n",
    "Acc_lr = accuracy_score(validation_labels, predicted_labels_lr)\n",
    "print('Accuracy rate for NB model: {:0.2f}%'.format(Acc_bayes*100))\n",
    "print('Accuracy rate for RandomForest model: {:0.2f}%'.format(Acc_ranfor*100))\n",
    "print('Accuracy rate for KNN model: {:0.2f}%'.format(Acc_knn*100))\n",
    "print('Accuracy rate for Logistic model: {:0.2f}%'.format(Acc_lr*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose Log Loss as our evaluation metric because it maps the real numbers to probability in the smoothest manner amking it useful when dealing with confidence, which we want to because we know that later on we will be using a voting model that takes this confidence into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_bayes = log_loss(validation_labels, predicted_probas_bayes)\n",
    "log_loss_ranfor = log_loss(validation_labels, predicted_probas_randomforest)\n",
    "log_loss_lr = log_loss(validation_labels, predicted_probas_lr)\n",
    "print('Error rate for Bayes model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_bayes*100))\n",
    "print('Error rate for Random Forest model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_ranfor*100))\n",
    "print('Error rate for Logistic Regression model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_lr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a problem here something about the dimension of the probability matrix, tried normalising, didn't fix it\n",
    "log_loss_knn = log_loss(validation_labels, predicted_probas_knn)\n",
    "print('Error rate for Logistic model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_knn*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both tests, Bayes seems to perform the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Applying Model to Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after training our model, we can now apply it to our test set.\n",
    "<br>\n",
    "<br>\n",
    "We applied each of the four models that we currently have and added them as columns to a dataframe for easy comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels_bayes = modelbayes.predict(tfidf_test)\n",
    "predicted_test_labels_rf = randomforest.predict(tfidf_test)\n",
    "predicted_test_labels_knn = modelknn.predict(tfidf_test)\n",
    "predicted_test_labels_lr = modellr.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_handle_test_onhold[\"Pred Labels Bayes\"] = predicted_test_labels_bayes\n",
    "df_handle_test_onhold[\"Pred Labels RF\"] = predicted_test_labels_rf\n",
    "df_handle_test_onhold[\"Pred Labels KNN\"] = predicted_test_labels_knn\n",
    "df_handle_test_onhold[\"Pred Labels LR\"] = predicted_test_labels_lr\n",
    "df_handle_test_onhold.to_csv('predicted labels.csv')\n",
    "df_handle_test_onhold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first wanted to see if all of our models agreed on their class label for each document. To do this, we compared where the models agreed and where they disagreed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_agree = 0\n",
    "count_disagree = 0\n",
    "\n",
    "for index, row in df_handle_test_onhold.iterrows():\n",
    "    if row[\"Pred Labels Bayes\"]==row[\"Pred Labels RF\"]==row[\"Pred Labels KNN\"]==row[\"Pred Labels LR\"]:\n",
    "        count_agree+=1\n",
    "    else:\n",
    "        count_disagree+=1\n",
    "        \n",
    "print(\"Number of articles agreed on by the models: {:0.2f}%\".format((count_agree/df_handle_test_onhold.shape[0])*100) )\n",
    "print(\"Number of articles disagreed on by the models: {:0.2f}%\".format((count_disagree/df_handle_test_onhold.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the models agree with each other 70% of the time. Based on this we decided that we wanted to use a voting model to find a consensus on cases where the models disagreed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Classifier Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an unweighted voting system. It uses the previous models to create a voting classifier and we then fit this moel to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf_unweighted = VotingClassifier(estimators=[('nb', modelbayes), ('rf', randomforest), ('knn', modelknn), ('lr', modellr)], voting='soft')\n",
    "eclf_unweighted.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tested this model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_eclf_unweighted = eclf_unweighted.predict(validation_tfidf)\n",
    "predicted_probas_eclf_unweighted = eclf_unweighted.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we found the log loss of the predicted and actual labels based off this training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_eclf_unweighted = log_loss(validation_labels, predicted_probas_eclf_unweighted)\n",
    "print('Error rate for unweighted model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_eclf_unweighted*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a variety of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Bayes and Logistic Regression have performed the best, we weighted them to reflect that. After some trial and error we chose these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_values =[16, 1, 1, 6] # this is been chosen by trial and error, it would be great if we got a systematic method to get the best values\n",
    "eclf_weighted = VotingClassifier(estimators=[('nb', modelbayes), ('rf', randomforest), ('knn', modelknn), ('lr', modellr)], voting='soft', weights=weight_values)\n",
    "eclf_weighted.fit(model_tfidf, model_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_eclf_weighted = eclf_weighted.predict(validation_tfidf)\n",
    "predicted_probas_eclf_weighted = eclf_weighted.predict_proba(validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_eclf_weighted = log_loss(validation_labels, predicted_probas_eclf_weighted)\n",
    "print('Error rate for weighted model using Log Loss evaluation metric: {:0.2f}%'.format(log_loss_eclf_weighted*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error rate is much lower than our previous singular models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Final Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we added our final predicted class labels as a new column to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels_eclf_weighted = eclf_weighted.predict(tfidf_test)\n",
    "df_handle_test_onhold[\"Pred Labels ECLF-W\"] = predicted_test_labels_eclf_weighted\n",
    "df_handle_test_onhold.to_csv('predicted labels final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle_test_onhold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle_test_onhold[[\"content\", \"Pred Labels ECLF-W\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
