{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPETITION TASK: \n",
    "\n",
    "+ Learn the classification model for training set with 5 categorical data from ['business', 'entertainment', 'politics', 'sport', 'tech'].\n",
    "\n",
    "+ Apply learned model to get the labels for \"testdata.csv\"\n",
    "\n",
    "#### Team: \n",
    "Laura Brierton - 15317451, Clodagh Lalor - student#, Jeremy Schiff - student#, Peter Concannon - student#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category\n",
       "0  French boss to leave EADS The French co-head o...       business\n",
       "1  Gamers could drive high-definition TV, films, ...           tech\n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics\n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment\n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_trainset = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "raw_testdata = pd.read_csv('testdata.csv',sep='^',header=0)\n",
    "raw_trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Function to convert raw text to tokens\n",
    "def convert_tokens(rawtext, verbose):\n",
    "    # First: Tokenization\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_words = tokenizer.tokenize(rawtext)\n",
    "    if (verbose):\n",
    "        print('Tokens:' + str(token_words[0:10]))\n",
    "    \n",
    "    # # Second: Decapitalization (if needed)\n",
    "    # decap_token_words = [word.lower() for word in token_words]\n",
    "    # print('Decapitalized Tokens:' + str(decap_token_words[0:10]))\n",
    "    \n",
    "    # Third: Remove stop words\n",
    "    json_data=open('stopwords.json', encoding=\"utf8\").read()\n",
    "    stopwords_json = json.loads(json_data)\n",
    "    stopwords_json_en = set(stopwords_json['en'])\n",
    "    stopwords_nltk_en = set(stopwords.words('english'))\n",
    "    # Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "    stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en)\n",
    "\n",
    "    rmsw_token_words = ([word for word in token_words if word.lower() not in stoplist_combined])\n",
    "    if (verbose):\n",
    "        print('Stopwords removed:' + str(rmsw_token_words[0:20]))\n",
    "    \n",
    "    ## Fouth: remove CAP words\n",
    "    rmcap_token_words =[]\n",
    "    for word in rmsw_token_words:\n",
    "        if word.isupper():\n",
    "            rmcap_token_words.append(word.title())\n",
    "        else:\n",
    "            rmcap_token_words.append(word)\n",
    "    if (verbose):\n",
    "        print('CAPITALIZED removed:' + str(rmcap_token_words[0:20]))\n",
    "        \n",
    "     ## Fifth : Remove salutation\n",
    "    salutation = ['mr','mrs','mss','dr','phd','prof','rev', 'professor']\n",
    "    rmsalu_token_words = ([word for word in rmcap_token_words if word.lower() not in salutation])\n",
    "    if (verbose):\n",
    "        print('Salutation removed:' + str(rmsalu_token_words[0:20]))\n",
    "        \n",
    "     ## Sixth: Remove Numbers\n",
    "    rmnb_token_words = ([word for word in rmsalu_token_words if not word.isdigit()])\n",
    "    if (verbose):\n",
    "        print('Number removed: ' + str(rmnb_token_words[0:20]))\n",
    "        \n",
    "    ## define transfer tag function:\n",
    "    def transfer_tag(treebank_tag):\n",
    "        if treebank_tag.startswith('j' or 'J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('v' or 'V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('n' or 'N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('r' or 'R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return 'n'\n",
    "    \n",
    "    ## Seventh: Lemmatization\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(rmnb_token_words):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        if not wtag:\n",
    "            lemma_words.extend([word])\n",
    "        else:\n",
    "            lemma_words.extend([wnl.lemmatize(word, wtag)]) # -> get lemma for word with tag\n",
    "    if (verbose):\n",
    "        print('Lemmas : ' + str(lemma_words[0:10]))\n",
    "        \n",
    "    \n",
    "    ## RETURN\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "      <td>[French, bos, leave, Eads, French, head, Europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[Gamers, drive, high, definition, Tv, film, ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[Stalemate, pension, strike, talk, Talks, aim,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Johnny, Denise, lose, Passport, Johnny, Vaugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Tautou, star, Da, Vinci, film, French, actres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Media seek Jackson 'juror' notes Reporters cov...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Media, seek, Jackson, juror, note, Reporters,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Horror film heads US box office A low-budget h...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Horror, film, head, box, office, low, budget,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kerr frustrated at victory margin Republic of ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[Kerr, frustrate, victory, margin, Republic, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US casino 'tricks' face ban in UK Controversia...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[casino, trick, face, ban, Uk, Controversial, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Klinsmann issues Lehmann warning Germany coach...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[Klinsmann, issue, Lehmann, warn, Germany, coa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category  \\\n",
       "0  French boss to leave EADS The French co-head o...       business   \n",
       "1  Gamers could drive high-definition TV, films, ...           tech   \n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics   \n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment   \n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment   \n",
       "5  Media seek Jackson 'juror' notes Reporters cov...  entertainment   \n",
       "6  Horror film heads US box office A low-budget h...  entertainment   \n",
       "7  Kerr frustrated at victory margin Republic of ...          sport   \n",
       "8  US casino 'tricks' face ban in UK Controversia...       politics   \n",
       "9  Klinsmann issues Lehmann warning Germany coach...          sport   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [French, bos, leave, Eads, French, head, Europ...  \n",
       "1  [Gamers, drive, high, definition, Tv, film, ga...  \n",
       "2  [Stalemate, pension, strike, talk, Talks, aim,...  \n",
       "3  [Johnny, Denise, lose, Passport, Johnny, Vaugh...  \n",
       "4  [Tautou, star, Da, Vinci, film, French, actres...  \n",
       "5  [Media, seek, Jackson, juror, note, Reporters,...  \n",
       "6  [Horror, film, head, box, office, low, budget,...  \n",
       "7  [Kerr, frustrate, victory, margin, Republic, I...  \n",
       "8  [casino, trick, face, ban, Uk, Controversial, ...  \n",
       "9  [Klinsmann, issue, Lehmann, warn, Germany, coa...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle = raw_trainset.copy()\n",
    "[n,d] = df_handle.shape\n",
    "df_handle['Tokens'] = ['']*n\n",
    "\n",
    "for index, row in df_handle.iterrows():\n",
    "    df_handle['Tokens'].iloc[index] = convert_tokens(row['content'],0)\n",
    "    \n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
