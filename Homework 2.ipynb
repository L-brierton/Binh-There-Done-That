{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics - Assignment 2\n",
    "COMPETITION TASK: \n",
    "\n",
    "+ Learn the classification model for training set with 5 categorical data from ['business', 'entertainment', 'politics', 'sport', 'tech'].\n",
    "\n",
    "+ Apply learned model to get the labels for \"testdata.csv\"\n",
    "\n",
    "## Team Members: \n",
    "Laura Brierton - 15317451, Clodagh Lalor - 13354426, Jeremy Schiff - student#, Peter Concannon - student#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8f0fac0be342>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8f0fac0be342>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    %%javascript\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## we used the following notebook extension to create a table of contents that appears at the top of our script\n",
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category\n",
       "0  French boss to leave EADS The French co-head o...       business\n",
       "1  Gamers could drive high-definition TV, films, ...           tech\n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics\n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment\n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we started by importing our datasets\n",
    "raw_trainset = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "raw_testdata = pd.read_csv('testdata.csv',sep='^',header=0)\n",
    "raw_trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Function to convert raw text to tokens\n",
    "def convert_tokens(rawtext, verbose):\n",
    "    # First: Tokenization\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_words = tokenizer.tokenize(rawtext)\n",
    "    if (verbose):\n",
    "        print('Tokens:' + str(token_words[0:10]))\n",
    "    \n",
    "    # Second: Decapitalization \n",
    "    decap_token_words = [word.lower() for word in token_words]\n",
    "    if (verbose):\n",
    "        print('Decapitalized Tokens:' + str(decap_token_words[0:10]))\n",
    "    \n",
    "    # Third: Remove stop words\n",
    "    json_data=open('stopwords.json', encoding=\"utf8\").read()\n",
    "    stopwords_json = json.loads(json_data)\n",
    "    stopwords_json_en = set(stopwords_json['en'])\n",
    "    stopwords_nltk_en = set(stopwords.words('english'))\n",
    "    # Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "    stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en)\n",
    "\n",
    "    ##** Depending on whether we use the second step or not **\n",
    "    \n",
    "    rmsw_token_words = ([word for word in token_words if word.lower() not in stoplist_combined])\n",
    "    ##rmsw_token_words = ([word for word in decap_token_words if word.lower() not in stoplist_combined])\n",
    "    if (verbose):\n",
    "        print('Stopwords removed:' + str(rmsw_token_words[0:20]))\n",
    "    \n",
    "    ## Fouth: remove CAP words\n",
    "    rmcap_token_words =[]\n",
    "    for word in rmsw_token_words:\n",
    "        if word.isupper():\n",
    "            rmcap_token_words.append(word.title())\n",
    "        else:\n",
    "            rmcap_token_words.append(word)\n",
    "    if (verbose):\n",
    "        print('CAPITALIZED removed:' + str(rmcap_token_words[0:20]))\n",
    "        \n",
    "     ## Fifth : Remove salutation\n",
    "    salutation = ['mr','mrs','mss','dr','phd','prof','rev', 'professor']\n",
    "    rmsalu_token_words = ([word for word in rmcap_token_words if word.lower() not in salutation])\n",
    "    if (verbose):\n",
    "        print('Salutation removed:' + str(rmsalu_token_words[0:20]))\n",
    "        \n",
    "     ## Sixth: Remove Numbers\n",
    "    rmnb_token_words = ([word for word in rmsalu_token_words if not word.isdigit()])\n",
    "    if (verbose):\n",
    "        print('Number removed: ' + str(rmnb_token_words[0:20]))\n",
    "        \n",
    "    ## define transfer tag function:\n",
    "    def transfer_tag(treebank_tag):\n",
    "        if treebank_tag.startswith('j' or 'J'):\n",
    "            return 'a'\n",
    "        elif treebank_tag.startswith('v' or 'V'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('n' or 'N'):\n",
    "            return 'n'\n",
    "        elif treebank_tag.startswith('r' or 'R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return 'n'\n",
    "    \n",
    "    ## Seventh: Lemmatization\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(rmnb_token_words):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        if not wtag:\n",
    "            lemma_words.extend([word])\n",
    "        else:\n",
    "            lemma_words.extend([wnl.lemmatize(word, wtag)]) # -> get lemma for word with tag\n",
    "    if (verbose):\n",
    "        print('Lemmas : ' + str(lemma_words[0:10]))\n",
    "        \n",
    "    \n",
    "    ## RETURN\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "      <td>[French, bos, leave, Eads, French, head, Europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[Gamers, drive, high, definition, Tv, film, ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[Stalemate, pension, strike, talk, Talks, aim,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Johnny, Denise, lose, Passport, Johnny, Vaugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Tautou, star, Da, Vinci, film, French, actres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Media seek Jackson 'juror' notes Reporters cov...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Media, seek, Jackson, juror, note, Reporters,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Horror film heads US box office A low-budget h...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>[Horror, film, head, box, office, low, budget,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kerr frustrated at victory margin Republic of ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[Kerr, frustrate, victory, margin, Republic, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US casino 'tricks' face ban in UK Controversia...</td>\n",
       "      <td>politics</td>\n",
       "      <td>[casino, trick, face, ban, Uk, Controversial, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Klinsmann issues Lehmann warning Germany coach...</td>\n",
       "      <td>sport</td>\n",
       "      <td>[Klinsmann, issue, Lehmann, warn, Germany, coa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category  \\\n",
       "0  French boss to leave EADS The French co-head o...       business   \n",
       "1  Gamers could drive high-definition TV, films, ...           tech   \n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics   \n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment   \n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment   \n",
       "5  Media seek Jackson 'juror' notes Reporters cov...  entertainment   \n",
       "6  Horror film heads US box office A low-budget h...  entertainment   \n",
       "7  Kerr frustrated at victory margin Republic of ...          sport   \n",
       "8  US casino 'tricks' face ban in UK Controversia...       politics   \n",
       "9  Klinsmann issues Lehmann warning Germany coach...          sport   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [French, bos, leave, Eads, French, head, Europ...  \n",
       "1  [Gamers, drive, high, definition, Tv, film, ga...  \n",
       "2  [Stalemate, pension, strike, talk, Talks, aim,...  \n",
       "3  [Johnny, Denise, lose, Passport, Johnny, Vaugh...  \n",
       "4  [Tautou, star, Da, Vinci, film, French, actres...  \n",
       "5  [Media, seek, Jackson, juror, note, Reporters,...  \n",
       "6  [Horror, film, head, box, office, low, budget,...  \n",
       "7  [Kerr, frustrate, victory, margin, Republic, I...  \n",
       "8  [casino, trick, face, ban, Uk, Controversial, ...  \n",
       "9  [Klinsmann, issue, Lehmann, warn, Germany, coa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we next create a dataframe that contained the content category and bag of words for each document\n",
    "df_handle = raw_trainset.copy()\n",
    "[n,d] = df_handle.shape\n",
    "df_handle['Tokens'] = ['']*n\n",
    "\n",
    "for index, row in df_handle.iterrows():\n",
    "    df_handle['Tokens'].iloc[index] = convert_tokens(row['content'],0)\n",
    "    \n",
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our tokenisation we can see that capitals still appear, as a result, we need to convert to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Wordcloud for the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decided to create a wordcloud for the entire corpus, to get an idea of the most common words and if there was any common pattern. We thought that it might help us to decide if there were any more pre-processing steps we needed to take before moving onto our Analysis stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud function\n",
    "def wordcloudplot(tokens, name):\n",
    "    \n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    text2 = ' '.join(tokens)\n",
    "\n",
    "    wordcloud = WordCloud(width=1600, height=800).generate(text2)\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    # save to file if filename given\n",
    "    if name:\n",
    "        wordcloud.to_file(name)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "# combine all the tokens\n",
    "for index, row in df_handle.iterrows():\n",
    "    all_tokens = all_tokens + df_handle['Tokens'].iloc[index]\n",
    "#all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saves wordcloud of all tokens as file. Please note, this word cloud is for all tokens not just noun tokens\n",
    "wordcloudplot(all_tokens, 'img_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our word cloud, we can see immediately that a lot of verbs are present, this will not necessarily help us with our classification step and so this influenced us to look at noun tokens instead, going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Most Common Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of our analysis involved looking at bigrams and trigrams, which in turn will be a huge help to us when it comes to classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(tell, Bbc)</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(prime, minister)</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Tony, Blair)</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(year, ago)</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(chief, executive)</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bigram  freq\n",
       "0         (tell, Bbc)   246\n",
       "1   (prime, minister)   170\n",
       "2       (Tony, Blair)   127\n",
       "3         (year, ago)   127\n",
       "4  (chief, executive)   122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=2\n",
    "bigram = ngrams(all_tokens,n)\n",
    "bi_frequencies = nltk.FreqDist(bigram)\n",
    "dict_items =list(dict(bi_frequencies).items())\n",
    "#make a dataframe of the bigrams and their frequencies\n",
    "df_bigramFreq = pd.DataFrame(dict_items,columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "df_bigramFreq = df_bigramFreq.reset_index(drop=True)\n",
    "#show only top five\n",
    "df_bigramFreq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the gram is noun gram or not\n",
    "def IsNounGram(ngram):\n",
    "    if ('-pron-' in ngram) or ('t' in ngram):\n",
    "        return False\n",
    "    \n",
    "    first_type = ('JJ','JJR','JJS','NN','NNS','NNP','NNPS')\n",
    "    second_type = ('NN','NNS','NNP','NNPS')\n",
    "    tags = nltk.pos_tag(ngram,lang='eng')\n",
    "    if (tags[0][1] in first_type) and (tags[1][1] in second_type):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165940"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only show noun grams\n",
    "df_bigramFreq_filter = df_bigramFreq[df_bigramFreq.bigram.map(lambda x : IsNounGram(x))]\n",
    "df_bigramFreq_filter = df_bigramFreq_filter.reset_index(drop=True)\n",
    "#df_bigramFreq_filter.head(5)\n",
    "df_bigramFreq_filter['bigram'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that our top five have changed a little (as has the entire dataframe). We were concerned with the (tell, BBC) bigram, and so decided that we needed to add an additional step of checking that this appeared in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check that these ngrams actually appear together in the text!\n",
    "def CheckWordInText(word, Text):\n",
    "    if word in Text.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all texts\n",
    "full_corpus = ''\n",
    "# combine all the tokens\n",
    "for index, row in df_handle.iterrows():\n",
    "    full_corpus = full_corpus + df_handle['content'].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3554748"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Note this next cell is taking FOREVER, anyone have an alternative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_bigram = df_bigramFreq_filter.copy().head(5)\n",
    "\n",
    "exits_list = []\n",
    "for index, row in df_bigramFreq_filter.iterrows():\n",
    "    gram = row['bigram']\n",
    "    word = (' '.join(gram))\n",
    "    exits_list.append(CheckWordInText(word, full_corpus))\n",
    "    \n",
    "df_real_bigram['exist'] = exits_list\n",
    "df_real_bigram = df_real_bigram.loc[df_real_bigram.exist==True]\n",
    "df_real_bigram = df_real_bigram.reset_index(drop=True)\n",
    "\n",
    "df_real_bigram.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now do the same for trigrams. We decided to go no higher than n = 3, for the purposes of this corpus. WHY???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "trigram = ngrams(all_tokens,n)\n",
    "tri_frequencies = nltk.FreqDist(trigram)\n",
    "dict_items =list(dict(tri_frequencies).items())\n",
    "#make a dataframe of the trigrams and their frequencies\n",
    "df_trigramFreq = pd.DataFrame(dict_items,columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
    "df_trigramFreq = df_trigramFreq.reset_index(drop=True)\n",
    "\n",
    "#check for noun grams\n",
    "df_trigramFreq_filter = df_trigramFreq[df_trigramFreq.trigram.map(lambda x : IsNounGram(x))]\n",
    "df_trigramFreq_filter = df_trigramFreq_filter.reset_index(drop=True)\n",
    "#print (df_trigramFreq.head(5) == df_trigramFreq_filter.head(5))\n",
    "\n",
    "#check in text\n",
    "df_real_trigram = df_trigramFreq_filter.copy()\n",
    "\n",
    "exits_list = []\n",
    "for index, row in df_trigramFreq_filter.iterrows():\n",
    "    gram = row['trigram']\n",
    "    word = (' '.join(gram))\n",
    "    exits_list.append(CheckWordInText(word, full_corpus))\n",
    "    \n",
    "df_real_trigram['exist'] = exits_list\n",
    "df_real_trigram = df_real_trigram.loc[df_real_trigram.exist==True]\n",
    "df_real_trigram = df_real_trigram.reset_index(drop=True)\n",
    "\n",
    "#df_trigramFreq_filter.head(5)\n",
    "df_real_trigram.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments on this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the column used for \"tokens\" needs to be updated to use clean noun tokens. \n",
    "# change the next line once this is done: df_handle[\"noun_tokens\"]\n",
    "merged_tokens = [\" \".join(x) for x in df_handle[\"Tokens\"]]\n",
    "tfidf_vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_out = pd.DataFrame(tfidf_vectorizer.fit_transform(merged_tokens).todense(), columns = tfidf_vectorizer.get_feature_names())\n",
    "tfidf_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
